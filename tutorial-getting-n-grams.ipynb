{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8741067d5d269c94ac1ae91e19184cd608123bbf"
   },
   "source": [
    "### The order words are used in is important\n",
    "\n",
    "The order that words are used in text is not random. In English, for example, you can say \"the red apple\" but not \"apple red the\". The relationships between words in text is very complex. So complex, in fact, that there's an entire field of linguistics devoted to it: syntax. (You can learn more about syntax [here](https://pdfs.semanticscholar.org/ba33/a9656f43a6b7420a180bdeccd5be98fc05eb.pdf), if you're interested.)\n",
    "\n",
    "However, there is a relatively simply way of capturing some of these relationships between words, originally proposed by Warren Weaver. You can capture quite a bit of information by just looking at which words tend to show up next to each other more often. \n",
    "\n",
    "\n",
    "### What are ngrams? \n",
    "\n",
    "The general idea is that you can look at each pair (or triple, set of four, etc.) of words that occur next to each other. In a sufficently-large corpus, you're likely to see \"the red\" and \"red apple\" several times, but less likely to see \"apple red\" and \"red the\". This is useful to know if, for example, you're trying to figure out what someone is more likely to say to help decide between possible output for an automatic speech recognition system.\n",
    "\n",
    "These co-occuring words are known as \"n-grams\", where \"n\" is a number saying how long a string of words you considered. (Unigrams are single words, bigrams are two words, trigrams are three words, 4-grams are four words, 5-grams are five words, etc.)\n",
    "\n",
    "\n",
    "> **Learning goal:** In this tutorial, you'll learn how to find all the n-grams in a corpus & count how often each is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "99e0adf8-2672-438d-9641-66c2fe4651b2",
    "_uuid": "394b7ec02251c51466dba97e843c0d2fd90cd043"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<doc id=\"62546\" title=\"Pragmática Sanción de 1713\" nonfiltered=\"1\" processed=\"1\" dbindex=\"25000\">\\nPragmática Sanción de 1713, norma promulgada por el emperador Carlos VI, de la Casa de Austria, en 1713 que facilitó la futura entronización de su hija María Teresa I. Ésta fue la primera ley fundamental común para todas las zonas de los Habsburgo, se decreto con un intento del emperador por lograr la integracion de los territorios del sacro imperio; sin embargo, el proyecto unificador no pudo alcanzarse del todo, pues Hungría puso como condicon para aceptar la Pragmática Sanción que fuera ratificada su constitucion y autonomia, lo que en realidad fortalecio el separatismo Hungaro.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nENDOFARTICLE.\\n</doc>\\n<doc id=\"62547\" title=\"Universidad Complutense de Madrid\" nonfiltered=\"2\" processed=\"2\" dbindex=\"25001\">\\n\\n\\nLa Universidad Complutense de Madrid (UCM) es una importante universidad española localizada en la Ciudad Universitaria de Madrid, España.\\n\\nHistoria.\\n\\nLos orígenes de la U'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in all the modules we're going to need\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "\n",
    "# this corpus is pretty big, so let's look at just one of the files in it\n",
    "with open(\"C:/Users/nosre/Downloads/spanishText_25000_30000\", \"r\",encoding='latin-1') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "\n",
    "# check to make sure the file read in alright; let's print out the first 2000 characters\n",
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "430143b5f28c56d6fb20747d4e4b1bf7d14f6eb7"
   },
   "source": [
    "Looking at the text, you can see that there's some extra xml markup that we don't really want to analyze (the suff in <pointy brackets>). Let's get rid of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "60a4f934a550fe8d033168a96e2337f3ceca4eaa",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pragmática Sanción de 1713 norma promulgada por el emperador Carlos VI de la Casa de Austria en 1713 que facilitó la futura entronización de su hija María Teresa I Ésta fue la primera ley fundamental común para todas las zonas de los Habsburgo se decreto con un intento del emperador por lograr la integracion de los territorios del sacro imperio sin embargo el proyecto unificador no pudo alcanzarse del todo pues Hungría puso como condicon para aceptar la Pragmática Sanción que fuera ratificada su constitucion y autonomia lo que en realidad fortalecio el separatismo HungaroLa Universidad Complutense de Madrid UCM es una importante universidad española localizada en la Ciudad Universitaria de Madrid EspañaHistoriaLos orígenes de la Universidad Complutense se encuentran en Universidad Cisneriana de Alcalá de Henares Tras languidecer durante el siglo XVIII mediante Real Orden de la Reina Regente de 29 de octubre de 1836 se decretó su supresión en Alcalá y traslado a Madrid donde pasó a deno'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do some preprocessing. We don't care about the XML notation, new lines \n",
    "# or punctuation marks other than periods. (We'll consider the end of the sentence\n",
    "# a \"word\") We also don't want to consider capitalization. \n",
    "\n",
    "# get rid of all the XML markup\n",
    "text = re.sub('<.*>','',text)\n",
    "\n",
    "#cambio get rid of /n\n",
    "text = re.sub('\\n','',text)\n",
    "\n",
    "# get rid of the \"ENDOFARTICLE.\" text\n",
    "text = re.sub('ENDOFARTICLE.','',text)\n",
    "\n",
    "# get rid of punctuation (except periods!)\n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "text = re.sub(punctuationNoPeriod, \"\", text)\n",
    "\n",
    "# make sure it looks ok\n",
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28d2b6735cde4eb44e95a47aea2016228e02e5f8"
   },
   "source": [
    "Ok, now onto making the n-grams! The first thing we want to do is \"tokenize\", or break the  text into indvidual words (you can find more information on tokenization [here](https://www.kaggle.com/rtatman/tokenization-tutorial))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "92145cdfb644377865c25c943f07449c16792cfe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first get individual words\n",
    "tokenized = text.split()\n",
    "\n",
    "# get list of all the trigrams\n",
    "esGrams = ngrams(tokenized,1)\n",
    "\n",
    "# and get a list of all the bi-grams\n",
    "esBigrams = ngrams(tokenized, 2)\n",
    "\n",
    "# get list of all the trigrams\n",
    "esTrigrams = ngrams(tokenized,3)\n",
    "\n",
    "\n",
    "# If you like, you can uncomment the next like to take a look at \n",
    "# the first ten to make sure they look ok. Please note that doing so \n",
    "# will consume the generator & will break the next block of code, so you'll\n",
    "# need to re-comment it and run this block again to get it to work.\n",
    "# list(esBigrams)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4990722239e09596e6eb63223bcd0d06cb284d9"
   },
   "source": [
    "Yay, we've got our n-grams! Just a list of all the bigrams in a text isn't that useful, though. Let's collapse it a little bit & get a count of how frequently we see each bigram in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a998670747c5c38314c95d517b3896122e999485",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('de', 'la'), 38923),\n",
       " (('en', 'el'), 16435),\n",
       " (('de', 'los'), 15828),\n",
       " (('en', 'la'), 15291),\n",
       " (('a', 'la'), 11038),\n",
       " (('de', 'las'), 9316),\n",
       " (('que', 'se'), 7004),\n",
       " (('y', 'el'), 5286),\n",
       " (('y', 'la'), 5111),\n",
       " (('a', 'los'), 4966),\n",
       " (('por', 'el'), 4939),\n",
       " (('con', 'el'), 4608),\n",
       " (('de', 'un'), 4290),\n",
       " (('de', 'su'), 4043),\n",
       " (('con', 'la'), 3955),\n",
       " (('en', 'los'), 3953),\n",
       " (('por', 'la'), 3781),\n",
       " (('lo', 'que'), 3458),\n",
       " (('de', 'una'), 3395),\n",
       " (('que', 'el'), 3339),\n",
       " (('en', 'las'), 3116),\n",
       " (('la', 'ciudad'), 3064),\n",
       " (('que', 'la'), 2991),\n",
       " (('en', 'un'), 2746),\n",
       " (('en', 'su'), 2730),\n",
       " (('es', 'el'), 2701),\n",
       " (('y', 'de'), 2605),\n",
       " (('y', 'en'), 2568),\n",
       " (('a', 'las'), 2510),\n",
       " (('es', 'un'), 2473)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the frequency of each bigram in our corpus\n",
    "esBigramFreq = collections.Counter(esBigrams)\n",
    "\n",
    "# what are the ten most popular ngrams in this Spanish corpus?\n",
    "esBigramFreq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('uno', 'de', 'los'), 1581),\n",
       " (('de', 'la', 'ciudad'), 1316),\n",
       " (('una', 'de', 'las'), 1116),\n",
       " (('la', 'ciudad', 'de'), 990),\n",
       " (('por', 'lo', 'que'), 936),\n",
       " (('a', 'través', 'de'), 881),\n",
       " (('el', 'nombre', 'de'), 737),\n",
       " (('parte', 'de', 'la'), 711),\n",
       " (('en', 'el', 'que'), 707),\n",
       " (('en', 'la', 'que'), 673),\n",
       " (('a', 'partir', 'de'), 637),\n",
       " (('la', 'mayoría', 'de'), 609),\n",
       " (('de', 'la', 'provincia'), 602),\n",
       " (('la', 'provincia', 'de'), 592),\n",
       " (('de', 'la', 'República'), 571),\n",
       " (('en', 'el', 'año'), 562),\n",
       " (('a', 'lo', 'largo'), 539),\n",
       " (('de', 'octubre', 'de'), 491),\n",
       " (('en', 'el', 'siglo'), 485),\n",
       " (('la', 'Universidad', 'de'), 483)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esTrigramFreq = collections.Counter(esTrigrams)\n",
    "\n",
    "esTrigramFreq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('de',), 262457),\n",
       " (('la',), 133828),\n",
       " (('en',), 98541),\n",
       " (('el',), 93087),\n",
       " (('y',), 92372),\n",
       " (('que',), 66878),\n",
       " (('a',), 60677),\n",
       " (('los',), 52295),\n",
       " (('del',), 49278),\n",
       " (('se',), 42588),\n",
       " (('por',), 33389),\n",
       " (('un',), 32902),\n",
       " (('las',), 31993),\n",
       " (('con',), 30297),\n",
       " (('una',), 28447),\n",
       " (('su',), 23918),\n",
       " (('es',), 22440),\n",
       " (('como',), 19584),\n",
       " (('al',), 19269),\n",
       " (('para',), 19124)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esGramFreq = collections.Counter(esGrams)\n",
    "\n",
    "esGramFreq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4133a80b9229a4f829b5174f91492bb63f64f64c"
   },
   "source": [
    "I'm not a fluent Spanish speaker, but those do look like some reasonable very frequent two-word phrases in Spanish.\n",
    "\n",
    "And that's all there is to it! Here are some exercises to help you try it out yourself.\n",
    "\n",
    "\n",
    "### Your turn!\n",
    "\n",
    "1. Find the most frequent n-grams in another file in this corpus. You can find the other files by entering \"../input/spanish_corpus/\" in a code chunk and then hitting the Tab key. All of the files will be listed in a drop-down menu. Are the most frequent bigrams the same as they were in this file?\n",
    "2. Instead of bigrams (two word phrases), can you find trigrams (three words)?\n",
    "3. Find the most frequent ngrams in another corpus. You can find some [here](https://www.kaggle.com/datasets?sortBy=relevance&group=featured&search=corpus) to start you out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
